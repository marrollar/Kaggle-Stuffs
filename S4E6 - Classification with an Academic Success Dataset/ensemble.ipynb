{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8975"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import umap\n",
    "import datetime\n",
    "import optuna\n",
    "import pprint\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, label_binarize, PolynomialFeatures, RobustScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import r2_score, accuracy_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import xgboost\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"data/test.csv\", index_col=0)\n",
    "original_df = pd.read_csv(\"data/original.csv\",sep=\";\")\n",
    "train_features = test_df.columns\n",
    "\n",
    "cat_features = ['Marital status', 'Application mode', 'Course',\n",
    "                'Previous qualification', 'Nacionality', \"Mother's qualification\", \n",
    "                \"Father's qualification\", \"Mother's occupation\",\n",
    "                \"Father's occupation\"]\n",
    "cont_features = list(set(train_features).difference(cat_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in cat_features:\n",
    "    dtype = pd.CategoricalDtype(categories=list(set(train_df[feat]) | set(test_df[feat]) | set(original_df[feat])), ordered=False)\n",
    "    for df in [train_df, test_df, original_df]:\n",
    "        df[feat] = df[feat].astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df, original_df]:\n",
    "    df[\"Misery Index\"] = df[\"Unemployment rate\"] + df[\"Inflation rate\"]\n",
    "    # df[\"Economic Discomfort\"] = df[\"Misery Index\"] - df[\"GDP\"]\n",
    "\n",
    "cont_features.append(\"Misery Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_order = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"Curricular units 2nd sem (approved)\": 0.5262597,\n",
    "        \"Tuition fees up to date\": 0.12040115,\n",
    "        \"Scholarship holder\": 0.04374109,\n",
    "        \"Curricular units 1st sem (approved)\": 0.043124773,\n",
    "        \"Curricular units 2nd sem (enrolled)\": 0.034864154,\n",
    "        \"Curricular units 2nd sem (evaluations)\": 0.033007413,\n",
    "        \"Curricular units 1st sem (evaluations)\": 0.026952056,\n",
    "        \"Debtor\": 0.02125777,\n",
    "        \"Curricular units 2nd sem (grade)\": 0.017488716,\n",
    "        \"Gender\": 0.015914941,\n",
    "        \"Age at enrollment\": 0.009499201,\n",
    "        \"Daytime/evening attendance\": 0.009322386,\n",
    "        \"Curricular units 1st sem (enrolled)\": 0.008687382,\n",
    "        \"Curricular units 2nd sem (credited)\": 0.008607062,\n",
    "        \"Course\": 0.008552427,\n",
    "        \"Mother's occupation\": 0.0074276286,\n",
    "        \"Application mode\": 0.007248743,\n",
    "        \"GDP\": 0.0058370973,\n",
    "        \"Curricular units 1st sem (grade)\": 0.00579759,\n",
    "        \"Unemployment rate\": 0.0038729862,\n",
    "        \"Mother's qualification\": 0.0035413294,\n",
    "        \"Displaced\": 0.0034240438,\n",
    "        \"Previous qualification\": 0.0033880775,\n",
    "        \"Father's occupation\": 0.0033660706,\n",
    "        \"Curricular units 1st sem (credited)\": 0.0032424054,\n",
    "        \"Admission grade\": 0.0031858524,\n",
    "        \"Curricular units 1st sem (without evaluations)\": 0.0031376902,\n",
    "        \"Marital status\": 0.0027686018,\n",
    "        \"Father's qualification\": 0.002645827,\n",
    "        \"Nacionality\": 0.0026041167,\n",
    "        \"Application order\": 0.0025012011,\n",
    "        \"Previous qualification (grade)\": 0.0024294835,\n",
    "        \"Inflation rate\": 0.0023359724,\n",
    "        \"International\": 0.0020073373,\n",
    "        \"Curricular units 2nd sem (without evaluations)\": 0.0015576994,\n",
    "    },\n",
    "    orient=\"index\",\n",
    "    columns=[\"val\"],\n",
    ")\n",
    "\n",
    "low_freq_cols = [\n",
    "    \"Nacionality\",\n",
    "    \"Educational special needs\",\n",
    "    \"International\",\n",
    "    \"Curricular units 1st sem (credited)\",\n",
    "    \"Curricular units 1st sem (without evaluations)\",\n",
    "    \"Curricular units 2nd sem (credited)\",\n",
    "    \"Curricular units 2nd sem (without evaluations)\",\n",
    "]\n",
    "\n",
    "def prune_low_freq(inp_df):\n",
    "    return inp_df.drop(columns=low_freq_cols, errors=\"ignore\")\n",
    "\n",
    "def prune_low_importance(inp_df, thresh):\n",
    "    pruned_cols = list(\n",
    "        feature_importances_order.loc[\n",
    "            feature_importances_order[\"val\"] < thresh\n",
    "        ].index\n",
    "    )\n",
    "    return inp_df.drop(columns=pruned_cols, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_study = optuna.load_study(\n",
    "    storage=\"sqlite:///optuna.sqlite3\",\n",
    "    study_name=\"aug_xgb_v1\",\n",
    ")\n",
    "rf_study = optuna.load_study(\n",
    "    storage=\"sqlite:///optuna.sqlite3\",\n",
    "    study_name=\"aug_rf_v1\",\n",
    ")\n",
    "logreg_study = optuna.load_study(\n",
    "    storage=\"sqlite:///optuna.sqlite3\",\n",
    "    study_name=\"aug_logreg_v2\",\n",
    ")\n",
    "\n",
    "untuned_xgb = xgboost.XGBClassifier(enable_categorical=True, n_jobs=-1)\n",
    "untuned_rf = RandomForestClassifier(n_jobs=-1)\n",
    "untuned_logreg = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "untuned_with_orig_xgb = xgboost.XGBClassifier(enable_categorical=True, n_jobs=-1)\n",
    "untuned_with_orig_rf = RandomForestClassifier(n_jobs=-1)\n",
    "untuned_with_orig_logreg = LogisticRegression(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'include_orig': False,\n",
      " 'prune_low_freq': True,\n",
      " 'prune_low_importance': False,\n",
      " 'poly_feats': False,\n",
      " 'use_standardscaler': False,\n",
      " 'use_robustscaler': False,\n",
      " 'classifier': 'xgboost',\n",
      " 'n_estimators': 1883,\n",
      " 'eta': 0.11299665616020274,\n",
      " 'gamma': 0.9634286209401426,\n",
      " 'max_depth': 3,\n",
      " 'max_leaves': 1346,\n",
      " 'colsample_bytree': 0.8385856672697529,\n",
      " 'colsample_bylevel': 0.8074747607390849,\n",
      " 'colsample_bynode': 0.7064701846488642,\n",
      " 'reg_lambda': 3,\n",
      " 'reg_alpha': 8,\n",
      " 'grow_policy': 'lossguide',\n",
      " 'min_child_weight': 70.27952002620962,\n",
      " 'max_delta_step': 94.63438008721305}\n",
      "\n",
      "{'include_orig': True,\n",
      " 'prune_low_freq': False,\n",
      " 'prune_low_importance': False,\n",
      " 'classifier': 'random_forest',\n",
      " 'n_estimators': 3373,\n",
      " 'max_depth': 6,\n",
      " 'min_samples_split': 0.022853786556543416,\n",
      " 'min_samples_leaf': 0.00020242539544159874,\n",
      " 'min_weight_fraction_leaf': 5.455836606089006e-05,\n",
      " 'max_features': 0.8642355014257856}\n",
      "\n",
      "{'include_orig': False,\n",
      " 'prune_low_freq': False,\n",
      " 'prune_low_importance': False,\n",
      " 'poly_feats': True,\n",
      " 'poly_feats_degrees': 2,\n",
      " 'use_standardscaler': False,\n",
      " 'use_robustscaler': False,\n",
      " 'classifier': 'logreg',\n",
      " 'penalty': 'l1',\n",
      " 'logreg_c': 19}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pprint.pp(xgb_study.best_params)\n",
    "tuned_xgb = xgboost.XGBClassifier(\n",
    "    enable_categorical=True,\n",
    "    n_jobs=-1,\n",
    "\n",
    "    n_estimators= xgb_study.best_params[\"n_estimators\"],\n",
    "    eta= xgb_study.best_params[\"eta\"],\n",
    "    gamma= xgb_study.best_params[\"gamma\"],\n",
    "    max_depth= xgb_study.best_params[\"max_depth\"],\n",
    "    max_leaves= xgb_study.best_params[\"max_leaves\"],\n",
    "    colsample_bytree= xgb_study.best_params[\"colsample_bytree\"],\n",
    "    colsample_bylevel= xgb_study.best_params[\"colsample_bylevel\"],\n",
    "    colsample_bynode= xgb_study.best_params[\"colsample_bynode\"],\n",
    "    reg_lambda= xgb_study.best_params[\"reg_lambda\"],\n",
    "    reg_alpha= xgb_study.best_params[\"reg_alpha\"],\n",
    "    grow_policy= xgb_study.best_params[\"grow_policy\"],\n",
    "    min_child_weight= xgb_study.best_params[\"min_child_weight\"],\n",
    "    max_delta_step= xgb_study.best_params[\"max_delta_step\"]\n",
    ")\n",
    "print()\n",
    "pprint.pp(rf_study.best_params)\n",
    "tuned_rf = RandomForestClassifier(\n",
    "    n_jobs=-1,\n",
    "\n",
    "    n_estimators=rf_study.best_params[\"n_estimators\"],\n",
    "    max_depth=rf_study.best_params[\"max_depth\"],\n",
    "    min_samples_split=rf_study.best_params[\"min_samples_split\"],\n",
    "    min_samples_leaf=rf_study.best_params[\"min_samples_leaf\"],\n",
    "    min_weight_fraction_leaf=rf_study.best_params[\"min_weight_fraction_leaf\"],\n",
    "    max_features=rf_study.best_params[\"max_features\"]\n",
    ")\n",
    "print()\n",
    "pprint.pp(logreg_study.best_params)\n",
    "tuned_logreg = LogisticRegression(\n",
    "    n_jobs=-1,\n",
    "\n",
    "    penalty=logreg_study.best_params[\"penalty\"],\n",
    "    C=logreg_study.best_params[\"logreg_c\"],\n",
    "    solver=\"saga\",\n",
    "    max_iter=int(1e5)\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df[\"Target\"])\n",
    "\n",
    "x = train_df.drop(columns=[\"Target\"])\n",
    "y = label_encoder.transform(train_df[\"Target\"])\n",
    "\n",
    "train_df_with_orig = pd.concat([train_df, original_df], axis=0)\n",
    "x_combined = train_df_with_orig.drop(columns=[\"Target\"])\n",
    "y_combined = label_encoder.transform(train_df_with_orig[\"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Untuned XGB\n",
      "Training Untuned RF\n",
      "Training Untuned LogReg\n",
      "Training Untuned XGB with Original Dataset\n",
      "Training Untuned RF with Original Dataset\n",
      "Training Untuned LogReg with Original Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training Untuned XGB\")\n",
    "untuned_xgb.fit(x, y)\n",
    "print(\"Training Untuned RF\")\n",
    "untuned_rf.fit(x, y)\n",
    "print(\"Training Untuned LogReg\")\n",
    "untuned_logreg.fit(x, y)\n",
    "\n",
    "print(\"Training Untuned XGB with Original Dataset\")\n",
    "untuned_with_orig_xgb.fit(x_combined, y_combined)\n",
    "print(\"Training Untuned RF with Original Dataset\")\n",
    "untuned_with_orig_rf.fit(x_combined, y_combined)\n",
    "print(\"Training Untuned LogReg with Original Dataset\")\n",
    "untuned_with_orig_logreg.fit(x_combined, y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"untuned_xgb\":untuned_xgb, \n",
    "    \"untuned_rf\":untuned_rf, \n",
    "    \"untuned_logreg\":untuned_logreg, \n",
    "    \"untuned_with_orig_xgb\":untuned_with_orig_xgb, \n",
    "    \"untuned_with_orig_rf\":untuned_with_orig_rf, \n",
    "    \"untuned_with_orig_logreg\":untuned_with_orig_logreg\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pred = model.predict(test_df)\n",
    "    pred = label_encoder.inverse_transform(pred)\n",
    "    out_pd = pd.DataFrame(index=test_df.index)\n",
    "    out_pd[\"Target\"] = pred\n",
    "\n",
    "    out_pd.to_csv(f\"{name}.csv\", columns=[\"Target\"], index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_preds = None\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        ensemble_preds += model.predict_proba(test_df)\n",
    "    except TypeError:\n",
    "        ensemble_preds = model.predict_proba(test_df)\n",
    "\n",
    "ensemble_preds /= len(models)\n",
    "ensemble_preds = np.argmax(ensemble_preds, axis=1)\n",
    "ensemble_preds = label_encoder.inverse_transform(ensemble_preds)\n",
    "out_pd = pd.DataFrame(index=test_df.index)\n",
    "out_pd[\"Target\"] = ensemble_preds\n",
    "out_pd.to_csv(\"untuned_ensemble.csv\", columns=[\"Target\"], index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_preds = None\n",
    "for name, model in list(models.items())[:3]:\n",
    "    try:\n",
    "        ensemble_preds += model.predict_proba(test_df)\n",
    "    except TypeError:\n",
    "        ensemble_preds = model.predict_proba(test_df)\n",
    "\n",
    "ensemble_preds /= len(models)\n",
    "ensemble_preds = np.argmax(ensemble_preds, axis=1)\n",
    "ensemble_preds = label_encoder.inverse_transform(ensemble_preds)\n",
    "out_pd = pd.DataFrame(index=test_df.index)\n",
    "out_pd[\"Target\"] = ensemble_preds\n",
    "out_pd.to_csv(\"untuned_ensemble_no_orig.csv\", columns=[\"Target\"], index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_inp_df = train_df.copy()\n",
    "xgb_inp_df = prune_low_freq(xgb_inp_df)\n",
    "\n",
    "x = xgb_inp_df.drop(columns=[\"Target\"])\n",
    "y = label_encoder.fit_transform(xgb_inp_df[\"Target\"])\n",
    "\n",
    "xgb_test_df = test_df.copy()\n",
    "xgb_test_df = prune_low_freq(xgb_test_df)\n",
    "\n",
    "tuned_xgb.fit(x, y)\n",
    "tuned_xgb_preds = tuned_xgb.predict(xgb_test_df)\n",
    "tuned_xgb_preds = label_encoder.inverse_transform(tuned_xgb_preds)\n",
    "out_pd = pd.DataFrame(index=xgb_test_df.index)\n",
    "out_pd[\"Target\"] = tuned_xgb_preds\n",
    "out_pd.to_csv(\"tuned_xgb.csv\", columns=[\"Target\"], index=True)\n",
    "\n",
    "tuned_xgb_predproba = tuned_xgb.predict_proba(xgb_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_inp_df = train_df.copy()\n",
    "rf_inp_df = pd.concat([rf_inp_df, original_df], axis=0)\n",
    "\n",
    "x = rf_inp_df.drop(columns=[\"Target\"])\n",
    "y = rf_inp_df[\"Target\"]\n",
    "\n",
    "tuned_rf.fit(x, y)\n",
    "tuned_rf_preds = tuned_rf.predict(test_df)\n",
    "out_pd = pd.DataFrame(index=test_df.index)\n",
    "out_pd[\"Target\"] = tuned_rf_preds\n",
    "out_pd.to_csv(\"tuned_rf.csv\", columns=[\"Target\"], index=True)\n",
    "\n",
    "tuned_rf_predproba = tuned_rf.predict_proba(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_feats = Pipeline([(\"poly_feats\", PolynomialFeatures(2))])\n",
    "col_transformer = ColumnTransformer(\n",
    "    [(\"numerical_pipeline\", poly_feats, cont_features)],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "logreg_inp_df = train_df.copy()\n",
    "\n",
    "x = logreg_inp_df.drop(columns=[\"Target\"])\n",
    "x = col_transformer.fit_transform(x)\n",
    "y = logreg_inp_df[\"Target\"]\n",
    "\n",
    "test_x = col_transformer.transform(test_df)\n",
    "\n",
    "tuned_logreg.fit(x, y)\n",
    "tuned_logreg_preds = tuned_logreg.predict(test_x)\n",
    "out_pd = pd.DataFrame(index=test_df.index)\n",
    "out_pd[\"Target\"] = tuned_logreg_preds\n",
    "out_pd.to_csv(\"tuned_logreg.csv\", columns=[\"Target\"], index=True)\n",
    "\n",
    "tuned_logreg_predproba = tuned_logreg.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_preds = tuned_xgb_predproba + tuned_rf_predproba + tuned_logreg_predproba\n",
    "ensemble_preds /= 3\n",
    "\n",
    "ensemble_preds = np.argmax(ensemble_preds, axis=1)\n",
    "ensemble_preds = label_encoder.inverse_transform(ensemble_preds)\n",
    "out_pd = pd.DataFrame(index=test_df.index)\n",
    "out_pd[\"Target\"] = ensemble_preds\n",
    "out_pd.to_csv(\"tuned_ensemble.csv\", columns=[\"Target\"], index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl_torchgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
