{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "import sklearn.linear_model as sklm\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.pipeline as skpl\n",
    "import sklearn.preprocessing as skpp\n",
    "import sklearn.compose as skcmp\n",
    "import category_encoders as ce\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from preprocessing import *\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programming\\Workspaces\\Python3 Workspace\\Misc Data Analysis\\Kaggle Playground Competition\\S4E8\\preprocessing.py:17: UserWarning: 'class' not found in dataframe\n",
      "  warnings.warn(f\"'{c}' not found in dataframe\")\n",
      "d:\\Programming\\Workspaces\\Python3 Workspace\\Misc Data Analysis\\Kaggle Playground Competition\\S4E8\\preprocessing.py:31: UserWarning: 'class' not found in dataframe\n",
      "  warnings.warn(f\"'{c}' not found in dataframe\")\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\", index_col=\"id\")\n",
    "test_df = pd.read_csv(\"data/test.csv\", index_col=\"id\")\n",
    "orig_df = fetch_ucirepo(id=848)['data']['original']\n",
    "\n",
    "CONT_FEATS = [\n",
    "    \"cap-diameter\",\n",
    "    \"stem-height\",\n",
    "    \"stem-width\"\n",
    "]\n",
    "CAT_FEATS = [c for c in train_df.columns if c not in CONT_FEATS]\n",
    "RESPONSE_COL = \"class\"\n",
    "\n",
    "train_df = convert_cols(train_df, CONT_FEATS, CAT_FEATS)\n",
    "test_df = convert_cols(test_df, CONT_FEATS, CAT_FEATS)\n",
    "orig_df = convert_cols(orig_df, CONT_FEATS, CAT_FEATS)\n",
    "\n",
    "train_df = null_all_non_original_categories(train_df, orig_df, CAT_FEATS)\n",
    "test_df = null_all_non_original_categories(test_df, orig_df, CAT_FEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, orig_df]:\n",
    "    df[\"class\"] = df[\"class\"].cat.rename_categories({\"e\":0, \"p\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_save_train(model, path, save_pred_proba=True):\n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(train_df.drop(columns=[\"class\"]), train_df[\"class\"])\n",
    "    \n",
    "    print(\"Getting predictions...\")\n",
    "    train_preds = model.predict(train_df.drop(columns=[\"class\"]))\n",
    "    if save_pred_proba:\n",
    "        pred_proba = model.predict_proba(train_df.drop(columns=[\"class\"]))\n",
    "        \n",
    "    print(\"Saving predictions\")\n",
    "    out_df = pd.DataFrame({\"class\":train_preds}, index=train_df.index)\n",
    "    out_df[\"class\"] = out_df[\"class\"].replace({0:\"e\", 1:\"p\"})\n",
    "    if save_pred_proba:\n",
    "        out_df[\"pred_proba_0\"] = pred_proba[:, 0]\n",
    "        out_df[\"pred_proba_1\"] = pred_proba[:, 1]\n",
    "    \n",
    "        \n",
    "    out_df.to_csv(path)\n",
    "\n",
    "def predict_save_test(model, path, save_pred_proba=True):\n",
    "    print(\"Getting predictions...\")\n",
    "    test_preds = model.predict(test_df)\n",
    "    if save_pred_proba:\n",
    "        pred_proba = model.predict_proba(test_df)\n",
    "        \n",
    "    print(\"Saving predictions\")\n",
    "    out_df = pd.DataFrame({\"class\":test_preds}, index=test_df.index)\n",
    "    out_df[\"class\"] = out_df[\"class\"].replace({0:\"e\", 1:\"p\"})\n",
    "    if save_pred_proba:\n",
    "        out_df[\"pred_proba_0\"] = pred_proba[:, 0]\n",
    "        out_df[\"pred_proba_1\"] = pred_proba[:, 1]\n",
    "    \n",
    "        \n",
    "    out_df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Getting predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programming\\Languages\\Python Venvs\\dgl_torchgeo\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [18:55:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions\n",
      "Getting predictions...\n",
      "Saving predictions\n"
     ]
    }
   ],
   "source": [
    "study = optuna.load_study(\n",
    "    storage=\"sqlite:///optuna.sqlite3\",\n",
    "    study_name=\"xgb_exploration\",\n",
    ")\n",
    "best_model = xgb.XGBClassifier(\n",
    "    enable_categorical=True,\n",
    "    device=\"cuda\",\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    "    **study.best_params\n",
    ")\n",
    "fit_predict_save_train(best_model, \"ensemble_data/train_simple_xgb_100.csv\")\n",
    "predict_save_test(best_model, \"test_preds/test_simple_xgb_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "[LightGBM] [Info] Number of positive: 1705396, number of negative: 1411549\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 898\n",
      "[LightGBM] [Info] Number of data points in the train set: 3116945, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.547137 -> initscore=0.189110\n",
      "[LightGBM] [Info] Start training from score 0.189110\n",
      "Getting predictions...\n",
      "Saving predictions\n",
      "Getting predictions...\n",
      "Saving predictions\n"
     ]
    }
   ],
   "source": [
    "study = optuna.load_study(\n",
    "    storage=\"sqlite:///optuna.sqlite3\",\n",
    "    study_name=\"dart_tuning\",\n",
    ")\n",
    "best_model = lgb.LGBMClassifier(\n",
    "    boosting_type=\"dart\",\n",
    "    n_jobs=-1,\n",
    "\n",
    "    **study.best_params\n",
    ")\n",
    "fit_predict_save_train(best_model, \"ensemble_data/train_tuned_dart_100.csv\")\n",
    "predict_save_test(best_model, \"test_preds/test_tuned_dart_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "[LightGBM] [Info] Number of positive: 1705396, number of negative: 1411549\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 898\n",
      "[LightGBM] [Info] Number of data points in the train set: 3116945, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.547137 -> initscore=0.189110\n",
      "[LightGBM] [Info] Start training from score 0.189110\n",
      "Getting predictions...\n",
      "Saving predictions\n",
      "Getting predictions...\n",
      "Saving predictions\n"
     ]
    }
   ],
   "source": [
    "study = optuna.load_study(\n",
    "    storage=\"sqlite:///optuna.sqlite3\",\n",
    "    study_name=\"lgbm_tuning\",\n",
    ")\n",
    "best_model = lgb.LGBMClassifier(\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_jobs=-1,\n",
    "\n",
    "    **study.best_params\n",
    ")\n",
    "fit_predict_save_train(best_model, \"ensemble_data/train_tuned_lgbm_100.csv\")\n",
    "predict_save_test(best_model, \"test_preds/test_tuned_lgbm_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Getting predictions...\n",
      "Saving predictions\n",
      "Getting predictions...\n",
      "Saving predictions\n"
     ]
    }
   ],
   "source": [
    "study = optuna.load_study(\n",
    "    storage=\"sqlite:///optuna.sqlite3\",\n",
    "    study_name=\"xgb_tuning\",\n",
    ")\n",
    "best_model = xgb.XGBClassifier(\n",
    "    enable_categorical=True,\n",
    "    device=\"cuda\",\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    "    **study.best_params\n",
    ")\n",
    "fit_predict_save_train(best_model, \"ensemble_data/train_tuned_xgb_100.csv\")\n",
    "predict_save_test(best_model, \"test_preds/test_tuned_xgb_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10145"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = TabularDataset(train_df)\n",
    "test_data = TabularDataset(test_df)\n",
    "predictor = TabularPredictor.load(\"AutogluonModels/simple\")\n",
    "\n",
    "train_preds = predictor.predict(train_data)\n",
    "train_pp = predictor.predict_proba(train_data)\n",
    "out_pd = pd.DataFrame(index=train_df.index)\n",
    "out_pd[\"class\"] = list(train_preds)\n",
    "out_pd = pd.concat([out_pd, train_pp], axis=1).rename(columns={\"e\":\"pred_proba_0\", \"p\":\"pred_proba_1\"})\n",
    "out_pd.to_csv(\"ensemble_data/train_autogluon.csv\")\n",
    "\n",
    "test_preds = predictor.predict(test_data)\n",
    "test_pp = predictor.predict_proba(test_data)\n",
    "out_pd = pd.DataFrame(index=test_df.index)\n",
    "out_pd[\"class\"] = list(test_preds)\n",
    "out_pd = pd.concat([out_pd, test_pp], axis=1).rename(columns={\"e\":\"pred_proba_0\", \"p\":\"pred_proba_1\"})\n",
    "out_pd.to_csv(\"test_preds/test_autogluon.csv\")\n",
    "\n",
    "del train_data, test_data, predictor, train_preds, train_pp, test_preds, test_pp, out_pd\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl_torchgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
